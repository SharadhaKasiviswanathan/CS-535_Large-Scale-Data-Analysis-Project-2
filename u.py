import boto3  # type: ignore

client = boto3.client('emr')

client.run_job_flow(
    Name="ClusterShar",
    # AWS will probably create a bucket like this for you. Change it to
    # your log bucket to view logs
    LogUri="s3://aws-logs-064794820934-us-east-2/elasticmapreduce/",
    ReleaseLabel="emr-7.3.0",
    Applications=[{"Name": "Spark"}, {"Name": "Hadoop"}],
    # Adding the bootstrap action to install pandas
    # BootstrapActions=[
    #     {
    #         'Name': 'Install dependencies',
    #         'ScriptBootstrapAction': {
    #             'Path': 's3://sharadhakasi/scripts/install-pandas.sh'
    #         }
    #     }
    # ],
    Configurations=[
        {
            "Classification": "spark-env",
            "Configurations": [
                {
                    "Classification": "export",
                    "Properties": {
                        # Change this to an s3 bucket in your account to test your code
                        "PAGE_PAIRS_OUTPUT": "s3://sharadhakasi/mutual_links"
                        #"CS535_S3_WORKSPACE": "s3://sharadhakasi/mutual_links" -- to be given when executed
                        #"CS535_S3_WORKSPACE":"s3://sharadhakasi/wikipedia_components"
                    },
                },
            ],
        },
        {
            "Classification": "spark",
            "Properties": {
                "maximizeResourceAllocation": "true"
            }
        },
        {
            "Classification": "spark-defaults",
            "Properties": {
                "spark.dynamicAllocation.executorIdleTimeout": "10800s",
                "spark.log4jHotPatch.enabled": "false",
                "spark.rdd.compress": "true",
                "spark.rpc.message.maxSize": "512",
                "spark.serializer": "org.apache.spark.serializer.KryoSerializer"
            }
        },
        {
            "Classification": "core-site",
            "Properties": {
                "fs.s3n.multipart.uploads.split.size": "5368709120"
            }
        },
    ],
    Instances={
        'InstanceGroups': [
            {
                'Name': 'Primary',
                'Market': 'ON_DEMAND',
                'InstanceRole': 'MASTER',
                'InstanceType': 'r6gd.2xlarge',
                # This should always be 1, you only need a single primary
                'InstanceCount': 1,
            },
            {
                'Name': 'Workers',
                'Market': 'ON_DEMAND',
                'InstanceRole': 'CORE',
                'InstanceType': 'r6gd.2xlarge',
                'InstanceCount': 2,
            },
        ],
        'KeepJobFlowAliveWhenNoSteps': False,
        'TerminationProtected': False,
        'Ec2SubnetId': 'subnet-0d0ec0fdba8122c01',
        'Ec2KeyName': 'sharadhakasi_key',
        # Replace these with security groups generated by EMR the first
        # time you started your cluster
        'AdditionalMasterSecurityGroups': ['sg-04db5623aae08de70'],
        'AdditionalSlaveSecurityGroups': ['sg-0c59184a743789fd2'],
    },
    VisibleToAllUsers=True,
    # Replace these with your instance profile and your service role
    # that were automatically created by EMR
    JobFlowRole='arn:aws:iam::064794820934:instance-profile/AmazonEMR-InstanceProfile-20240925T211027',
    ServiceRole='arn:aws:iam::064794820934:role/service-role/AmazonEMR-ServiceRole-20240925T211045',
    Steps=[{
        "Name": "My First Spark Project",
        "ActionOnFailure": "TERMINATE_CLUSTER",
        "HadoopJarStep": {
            "Jar": "command-runner.jar",
            "Args": [
                "spark-submit",
                "--deploy-mode", "client",
                "--master", "yarn",
                "--py-files",
                "s3://sharadhakasi/code/project2/updatep2.py",
                "s3://sharadhakasi/code/project2/uep2.py",
                #"s3://sharadhakasi/code/project2/updatep2.py"  -- to be given while execution
                #"s3://sharadhakasi/code/project2/uep2.py"
            
            ],
        },
    }],
)
